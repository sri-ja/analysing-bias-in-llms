{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srija/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curating the prompts for each analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially the dimensions along which I want to perform analysis were decided.\n",
    "\n",
    "These were as follows \n",
    "- **Baseline:** With simple prompts, ideally without hints to ensure that model does not get biased by the prompt\n",
    "- **Temporal:** Prompts that involve time - for past, present and future\n",
    "- **Quantifier:** Prompts that hint to a quantifier to indicate the section of population that is to be associated with the token\n",
    "- **Framing:** Prompts that are framed with a positive, negative or neutral tone\n",
    "- **Perspective:** Prompts that are framed from the perspective of what people think - aimed to highlight stereotypes and biases\n",
    "\n",
    "Initially a set of normal prompts were created by prompting Gemini 2.0. Separate prompts were created for each subcategory and each prompt had a placeholder for the identity term and the token. For each prompt the placeholder for identity term was to be replaced with a specific term based on region or religion, while the token was to be used as the mask.\n",
    "\n",
    "Following that, the prompts were modified to incorporate the dimensions of interest. For example, for the temporal dimension, the prompts were modified to include past, present and future tense. For the quantifier dimension, the prompts were modified to include quantifiers like 'most', 'some', 'all' etc. For the framing dimension, the prompts were modified to include positive, negative and neutral framing. For the perspective dimension, the prompts were modified to include prompts that are framed from the perspective of what people think. This process was also carried out by Gemini 2.0\n",
    "\n",
    "I did not use the prompts that were provided in the template folder in the original dataset as I believed they might influence the because of them inherently having some quantifiers and bias terms present (eg: prefer, always, most likely, etc.)\n",
    "\n",
    "So, for all the dimensions, the initial set of prompts were created and stored in their respective folders in the ```prompts.json``` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating final prompts by replacing the identity terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_folder = '../../nlp-fairness-for-india'\n",
    "utils_folder = '../../utils'\n",
    "\n",
    "prompts_folder = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes = ['region', 'religion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_terms = {}\n",
    "\n",
    "for axis in axes:\n",
    "    id_terms[axis] = []\n",
    "    with open(f'{dataset_folder}/{axis}_idterms.tsv', 'r') as f:\n",
    "        for line in f:\n",
    "            id_terms[axis].append(line.strip())\n",
    "        id_terms[axis] = id_terms[axis][1:]\n",
    "        id_terms[axis] = [term[0].upper() + term[1:] for term in id_terms[axis]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in os.listdir(prompts_folder):\n",
    "    if os.path.isdir(f'{prompts_folder}/{experiment}'):\n",
    "        prompts = json.load(open(f'{prompts_folder}/{experiment}/prompts.json', 'r'))\n",
    "\n",
    "        for axis in axes:\n",
    "            modified_prompts = {}\n",
    "            for id_term in id_terms[axis]:\n",
    "                modified_prompts[id_term] = {}\n",
    "                for subcategory in prompts:\n",
    "                    modified_prompts[id_term][subcategory] = []\n",
    "                    for prompt in prompts[subcategory]:\n",
    "                        modified_prompts[id_term][subcategory].append(prompt.replace('[IDENTITY_TERM]', id_term))\n",
    "\n",
    "            json.dump(modified_prompts, open(f'{prompts_folder}/{experiment}/{axis}_dataset.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the masked language model and storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/muril-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_fill = pipeline('fill-mask', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'The [MASK] is a beautiful place.'\n",
    "mask_fill(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in os.listdir(prompts_folder):\n",
    "    if os.path.isdir(f'{prompts_folder}/{experiment}'):\n",
    "        for axis in axes:\n",
    "            dataset = json.load(open(f'{prompts_folder}/{experiment}/{axis}_dataset.json', 'r'))\n",
    "            results = {}\n",
    "\n",
    "            for id_term in tqdm(dataset):\n",
    "                results[id_term] = {}\n",
    "                for subcategory in dataset[id_term]:\n",
    "                    results[id_term][subcategory] = []\n",
    "                    for prompt in dataset[id_term][subcategory]:\n",
    "                        result = mask_fill(prompt)\n",
    "                        results[id_term][subcategory].append(result)\n",
    "\n",
    "            json.dump(results, open(f'{prompts_folder}/{experiment}/{axis}_results.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
