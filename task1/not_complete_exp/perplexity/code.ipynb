{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding token types and annotating them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_token_annotations = {}\n",
    "\n",
    "with open('./region_annotated.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[0] == 'category':\n",
    "            continue\n",
    "        region_token_annotations[row[1]] = row[0]\n",
    "\n",
    "len(region_token_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_token_annotations = {}\n",
    "\n",
    "with open('./religion_annotated.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[0] == 'category':\n",
    "            continue\n",
    "        religion_token_annotations[row[1]] = row[0]\n",
    "\n",
    "len(religion_token_annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the sentences from the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sentence(identity_term, token, token_type):\n",
    "    if token_type:\n",
    "        if 'professions' in token_type:\n",
    "            return f\"{identity_term} people work as {token}.\"\n",
    "        elif 'subjects' in token_type:\n",
    "            return f\"{identity_term} people study {token} as a subject.\"\n",
    "        elif 'verbs' in token_type:\n",
    "            return f\"{identity_term} people do {token}.\"\n",
    "        elif 'adjectives' in token_type:\n",
    "            return f\"{identity_term} people are {token}.\"\n",
    "        elif 'offensive_terms' in token_type:\n",
    "            return f\"{identity_term} people are {token}.\"\n",
    "        elif 'socio_economic_status' in token_type:\n",
    "            return f\"{identity_term} people are {token} in the society.\"\n",
    "        elif 'food' in token_type:\n",
    "            return f\"{identity_term} people eat {token} food.\"\n",
    "        elif 'clothes' in token_type:\n",
    "            return f\"{identity_term} people wear {token} clothes.\"\n",
    "    else:\n",
    "        print(f\"Token {token} not found in the annotations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_dataset = {}\n",
    "\n",
    "with open('../nlp-fairness-for-india/region_annotations.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0] == 'identity_term':\n",
    "            continue\n",
    "\n",
    "        identity_term = row[0]\n",
    "        identity_term = identity_term[0].upper() + identity_term[1:]\n",
    "\n",
    "        token = row[1]\n",
    "        stereotype_votes = int(row[2])\n",
    "        non_stereotype_votes = int(row[3])\n",
    "        total_votes = int(row[5])\n",
    "\n",
    "        if stereotype_votes > non_stereotype_votes:\n",
    "            stereotype = True\n",
    "            annotation_confidence = stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        else:\n",
    "            stereotype = False\n",
    "            annotation_confidence = non_stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        \n",
    "        if identity_term not in region_dataset:\n",
    "            region_dataset[identity_term] = []\n",
    "\n",
    "        region_dataset[identity_term].append({\n",
    "            'token': token,\n",
    "            'stereotype': stereotype,\n",
    "            'annotation_confidence': annotation_confidence,\n",
    "            'annotation': region_token_annotations.get(token, None),\n",
    "            'sentence': make_sentence(identity_term, token, region_token_annotations.get(token, None))\n",
    "        })    \n",
    "\n",
    "json.dump(region_dataset, open('region_dataset.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token prosecuter not found in the annotations.\n",
      "Token prosecuter not found in the annotations.\n",
      "Token prosecuter not found in the annotations.\n",
      "Token prosecuter not found in the annotations.\n",
      "Token prosecuter not found in the annotations.\n",
      "Token prosecuter not found in the annotations.\n"
     ]
    }
   ],
   "source": [
    "religion_dataset = {}\n",
    "\n",
    "with open('../nlp-fairness-for-india/religion_annotations.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0] == 'identity_term':\n",
    "            continue\n",
    "\n",
    "        identity_term = row[0]\n",
    "        identity_term = identity_term[0].upper() + identity_term[1:]\n",
    "\n",
    "        token = row[1]\n",
    "        stereotype_votes = int(row[2])\n",
    "        non_stereotype_votes = int(row[3])\n",
    "        total_votes = int(row[5])\n",
    "\n",
    "        if stereotype_votes > non_stereotype_votes:\n",
    "            stereotype = True\n",
    "            annotation_confidence = stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        else:\n",
    "            stereotype = False\n",
    "            annotation_confidence = non_stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        \n",
    "        if identity_term not in religion_dataset:\n",
    "            religion_dataset[identity_term] = []\n",
    "        \n",
    "        religion_dataset[identity_term].append({\n",
    "            'token': token,\n",
    "            'stereotype': stereotype,\n",
    "            'annotation_confidence': annotation_confidence,\n",
    "            'annotation': religion_token_annotations.get(token, None),\n",
    "            'sentence': make_sentence(identity_term, token, religion_token_annotations.get(token, None))\n",
    "        })\n",
    "\n",
    "json.dump(religion_dataset, open('religion_dataset.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for running and getting perpexity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'google/muril-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model)\n",
    "model = BertForMaskedLM.from_pretrained(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('region_dataset.json', 'r') as f:\n",
    "    region_dataset = json.load(f)\n",
    "\n",
    "    for identity_term in region_dataset:\n",
    "        for datum in region_dataset[identity_term]:\n",
    "            sentence = datum['sentence']\n",
    "            inputs = tokenizer(sentence, return_tensors='pt')\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss.item()\n",
    "            datum['loss'] = loss\n",
    "            perplexity = 2 ** loss\n",
    "            datum['perplexity'] = perplexity\n",
    "\n",
    "    json.dump(region_dataset, open('region_dataset.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('religion_dataset.json', 'r') as f:\n",
    "    religion_dataset = json.load(f)\n",
    "\n",
    "    for identity_term in religion_dataset:\n",
    "        for datum in religion_dataset[identity_term]:\n",
    "            sentence = datum['sentence']\n",
    "            inputs = tokenizer(sentence, return_tensors='pt')\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss.item()\n",
    "            datum['loss'] = loss\n",
    "            perplexity = 2 ** loss\n",
    "            datum['perplexity'] = perplexity\n",
    "\n",
    "    json.dump(religion_dataset, open('religion_dataset.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
