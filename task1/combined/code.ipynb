{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_idterms = []\n",
    "\n",
    "with open('../nlp-fairness-for-india/region_idterms.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        region_idterms.append(line.strip())\n",
    "    region_idterms = region_idterms[1:]\n",
    "    # capitalizing the first letter of each term\n",
    "    region_idterms = [term[0].upper() + term[1:] for term in region_idterms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "religion_idterms = []\n",
    "\n",
    "with open('../nlp-fairness-for-india/religion_idterms.tsv', 'r') as f:\n",
    "    for line in f:\n",
    "        religion_idterms.append(line.strip())\n",
    "    religion_idterms = religion_idterms[1:]\n",
    "    # capitalizing the first letter of each term\n",
    "    religion_idterms = [term[0].upper() + term[1:] for term in religion_idterms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = json.load(open(f'prompts.json')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = {}\n",
    "\n",
    "for region in region_idterms:\n",
    "    combined_dataset[region] = {}\n",
    "    for religion in religion_idterms:\n",
    "        combined_dataset[region][religion] = {}\n",
    "        for prompt_category in prompts:\n",
    "            combined_dataset[region][religion][prompt_category] = []\n",
    "            for prompt in prompts[prompt_category]:\n",
    "                combined_dataset[region][religion][prompt_category].append(prompt.replace('[IDENTITY_TERM]', region + ' ' + religion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets\n",
    "json.dump(combined_dataset, open('combined_dataset.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the masked language model and storing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/muril-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_fill = pipeline('fill-mask', model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = 'The [MASK] is a beautiful place.'\n",
    "mask_fill(test_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset = json.load(open('combined_dataset.json'))\n",
    "combined_results = {}\n",
    "\n",
    "for region in combined_dataset:\n",
    "    combined_results[region] = {}\n",
    "    for religion in tqdm(combined_dataset[region], desc=f'Religions in {region}'):\n",
    "        combined_results[region][religion] = {}\n",
    "        for prompt_category in combined_dataset[region][religion]:\n",
    "            combined_results[region][religion][prompt_category] = []\n",
    "            for prompt in combined_dataset[region][religion][prompt_category]:\n",
    "                result = mask_fill(prompt)\n",
    "                combined_results[region][religion][prompt_category].append(result)\n",
    "\n",
    "json.dump(combined_results, open('combined_results.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code that needs help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_list = 'combined_results.json'\n",
    "        file_path = os.path.join('./mlm_analysis', folder, files_list)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        results = {}\n",
    "        scores = {}\n",
    "\n",
    "        for identity_term_1 in data:\n",
    "            results[identity_term_1] = {}\n",
    "            scores[identity_term_1] = {}\n",
    "            for identity_term_2 in data[identity_term_1]:\n",
    "                results[identity_term_1][identity_term_2] = {}\n",
    "                scores[identity_term_1][identity_term_2] = {}\n",
    "                for subcategory in data[identity_term_1][identity_term_2]:\n",
    "                    subcategory_term = subcategory_map[subcategory]\n",
    "                    results[identity_term_1][identity_term_2][subcategory_term] = {}\n",
    "                    scores[identity_term_1][identity_term_2][subcategory_term] = {}\n",
    "                    for datum in data[identity_term_1][identity_term_2][subcategory]:\n",
    "                        for top_results in datum:\n",
    "                            token = top_results['token_str']\n",
    "                            score = top_results['score']\n",
    "                            if token not in results[identity_term_1][identity_term_2][subcategory_term]:\n",
    "                                results[identity_term_1][identity_term_2][subcategory_term][token] = 0\n",
    "                                scores[identity_term_1][identity_term_2][subcategory_term][token] = 0\n",
    "\n",
    "                            results[identity_term_1][identity_term_2][subcategory_term][token] += 1\n",
    "                            scores[identity_term_1][identity_term_2][subcategory_term][token] += score\n",
    "\n",
    "        # sort results by highest occurence\n",
    "        for identity_term_1 in results:\n",
    "            for identity_term_2 in results[identity_term_1]:\n",
    "                for subcategory in results[identity_term_1][identity_term_2]:\n",
    "                    results[identity_term_1][identity_term_2][subcategory] = {k: v for k, v in sorted(results[identity_term_1][identity_term_2][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "                    scores[identity_term_1][identity_term_2][subcategory] = {k: v for k, v in sorted(scores[identity_term_1][identity_term_2][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        with open(f'./mlm_analysis/{folder}/results/{files_list}', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        with open(f'./mlm_analysis/{folder}/results/scores_{files_list}', 'w') as f:\n",
    "            json.dump(scores, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
