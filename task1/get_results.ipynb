{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rough ground for testing things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './mlm_analysis/adv_dec/region_results.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for identity_term in data:\n",
    "    results[identity_term] = {}\n",
    "    scores[identity_term] = {}\n",
    "    for subcategory in data[identity_term]:\n",
    "        results[identity_term][subcategory] = {}\n",
    "        scores[identity_term][subcategory] = {}\n",
    "        for datum in data[identity_term][subcategory]:\n",
    "            for top_results in datum:\n",
    "                token = top_results['token_str']\n",
    "                score = top_results['score']\n",
    "                if token not in results[identity_term][subcategory]:\n",
    "                    results[identity_term][subcategory][token] = 0\n",
    "                    scores[identity_term][subcategory][token] = 0\n",
    "\n",
    "                results[identity_term][subcategory][token] += 1\n",
    "                scores[identity_term][subcategory][token] += score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort results by highest occurence\n",
    "for identity_term in results:\n",
    "    for subcategory in results[identity_term]:\n",
    "        results[identity_term][subcategory] = {k: v for k, v in sorted(results[identity_term][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "        scores[identity_term][subcategory] = {k: v for k, v in sorted(scores[identity_term][subcategory].items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiling the results for each of the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcategory_map = {\n",
    "    'professions': 'professions',\n",
    "    'subjects_of_study': 'subjects',\n",
    "    'action_verbs': 'verbs',\n",
    "    'behaviour_adjectives': 'adjectives',\n",
    "    'socio_economic_status_adjectives': 'socio_economic_status',\n",
    "    'food_habits_adjectives': 'food',\n",
    "    'clothing_preferences_adjectives': 'clothes',\n",
    "    'general': 'general'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_framing\n",
      "positive_framing\n",
      "adv_inc\n",
      "adv_perspective_shift\n",
      "adv_dec\n",
      "adv_future\n",
      "vanilla\n",
      "combined\n",
      "adv_present\n",
      "neutral_framing\n",
      "adv_past\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir('./mlm_analysis'):\n",
    "    print(folder)\n",
    "    os.makedirs(f'./mlm_analysis/{folder}/results', exist_ok=True)\n",
    "\n",
    "    if 'combined' not in folder:\n",
    "        files_list = ['region_results.json', 'religion_results.json']\n",
    "        for file in files_list:\n",
    "            file_path = os.path.join('./mlm_analysis', folder, file)\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            results = {}\n",
    "            scores = {}\n",
    "\n",
    "            for identity_term in data:\n",
    "                results[identity_term] = {}\n",
    "                scores[identity_term] = {}\n",
    "                for subcategory in data[identity_term]:\n",
    "                    subcategory_term = subcategory_map[subcategory]\n",
    "\n",
    "                    results[identity_term][subcategory_term] = {}\n",
    "                    scores[identity_term][subcategory_term] = {}\n",
    "                    for datum in data[identity_term][subcategory]:\n",
    "                        for top_results in datum:\n",
    "                            try:\n",
    "                                token = top_results['token_str'].lower()\n",
    "                                score = top_results['score']\n",
    "                            except:\n",
    "                                print(datum)\n",
    "\n",
    "                            if token not in results[identity_term][subcategory_term]:\n",
    "                                results[identity_term][subcategory_term][token] = 0\n",
    "                                scores[identity_term][subcategory_term][token] = 0\n",
    "\n",
    "                            results[identity_term][subcategory_term][token] += 1\n",
    "                            scores[identity_term][subcategory_term][token] += score\n",
    "\n",
    "            # sort results by highest occurence\n",
    "            for identity_term in results:\n",
    "                for subcategory in results[identity_term]:\n",
    "                    results[identity_term][subcategory] = {k: v for k, v in sorted(results[identity_term][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "                    scores[identity_term][subcategory] = {k: v for k, v in sorted(scores[identity_term][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "            with open(f'./mlm_analysis/{folder}/results/{file}', 'w') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "            with open(f'./mlm_analysis/{folder}/results/scores_{file}', 'w') as f:\n",
    "                json.dump(scores, f, indent=4)\n",
    "    else:\n",
    "        files_list = 'combined_results.json'\n",
    "        file_path = os.path.join('./mlm_analysis', folder, files_list)\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        results = {}\n",
    "        scores = {}\n",
    "\n",
    "        for identity_term_1 in data:\n",
    "            results[identity_term_1] = {}\n",
    "            scores[identity_term_1] = {}\n",
    "            for identity_term_2 in data[identity_term_1]:\n",
    "                results[identity_term_1][identity_term_2] = {}\n",
    "                scores[identity_term_1][identity_term_2] = {}\n",
    "                for subcategory in data[identity_term_1][identity_term_2]:\n",
    "                    subcategory_term = subcategory_map[subcategory]\n",
    "                    results[identity_term_1][identity_term_2][subcategory_term] = {}\n",
    "                    scores[identity_term_1][identity_term_2][subcategory_term] = {}\n",
    "                    for datum in data[identity_term_1][identity_term_2][subcategory]:\n",
    "                        for top_results in datum:\n",
    "                            token = top_results['token_str']\n",
    "                            score = top_results['score']\n",
    "                            if token not in results[identity_term_1][identity_term_2][subcategory_term]:\n",
    "                                results[identity_term_1][identity_term_2][subcategory_term][token] = 0\n",
    "                                scores[identity_term_1][identity_term_2][subcategory_term][token] = 0\n",
    "\n",
    "                            results[identity_term_1][identity_term_2][subcategory_term][token] += 1\n",
    "                            scores[identity_term_1][identity_term_2][subcategory_term][token] += score\n",
    "\n",
    "        # sort results by highest occurence\n",
    "        for identity_term_1 in results:\n",
    "            for identity_term_2 in results[identity_term_1]:\n",
    "                for subcategory in results[identity_term_1][identity_term_2]:\n",
    "                    results[identity_term_1][identity_term_2][subcategory] = {k: v for k, v in sorted(results[identity_term_1][identity_term_2][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "                    scores[identity_term_1][identity_term_2][subcategory] = {k: v for k, v in sorted(scores[identity_term_1][identity_term_2][subcategory].items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "        with open(f'./mlm_analysis/{folder}/results/{files_list}', 'w') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "        with open(f'./mlm_analysis/{folder}/results/scores_{files_list}', 'w') as f:\n",
    "            json.dump(scores, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding stereotype scores for the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_token_annotations = {}\n",
    "\n",
    "with open('./mlm_analysis/region_annotated.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[0] == 'category':\n",
    "            continue\n",
    "        region_token_annotations[row[1]] = row[0]\n",
    "\n",
    "len(region_token_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "religion_token_annotations = {}\n",
    "\n",
    "with open('./mlm_analysis/religion_annotated.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if row[0] == 'category':\n",
    "            continue\n",
    "        religion_token_annotations[row[1]] = row[0]\n",
    "\n",
    "len(religion_token_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_region_data = {}\n",
    "\n",
    "with open('./nlp-fairness-for-india/region_annotations.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0] == 'identity_term':\n",
    "            continue\n",
    "\n",
    "        identity_term = row[0]\n",
    "        identity_term = identity_term[0].upper() + identity_term[1:]\n",
    "\n",
    "        token = row[1]\n",
    "        stereotype_votes = int(row[2])\n",
    "        non_stereotype_votes = int(row[3])\n",
    "        total_votes = int(row[5])\n",
    "\n",
    "        if stereotype_votes > non_stereotype_votes:\n",
    "            stereotype = True\n",
    "            annotation_confidence = stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        else:\n",
    "            stereotype = False\n",
    "            annotation_confidence = non_stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        \n",
    "        if identity_term not in all_region_data:\n",
    "            all_region_data[identity_term] = []\n",
    "\n",
    "        all_region_data[identity_term].append({\n",
    "            'token': token,\n",
    "            'stereotype': stereotype,\n",
    "            'annotation_confidence': annotation_confidence,\n",
    "            'annotation': region_token_annotations.get(token, None),\n",
    "        })    \n",
    "\n",
    "json.dump(all_region_data, open('./mlm_analysis/all_region_data.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_religion_data = {}\n",
    "\n",
    "with open('./nlp-fairness-for-india/religion_annotations.tsv', 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        if row[0] == 'identity_term':\n",
    "            continue\n",
    "\n",
    "        identity_term = row[0]\n",
    "        identity_term = identity_term[0].upper() + identity_term[1:]\n",
    "\n",
    "        token = row[1]\n",
    "        stereotype_votes = int(row[2])\n",
    "        non_stereotype_votes = int(row[3])\n",
    "        total_votes = int(row[5])\n",
    "\n",
    "        if stereotype_votes > non_stereotype_votes:\n",
    "            stereotype = True\n",
    "            annotation_confidence = stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        else:\n",
    "            stereotype = False\n",
    "            annotation_confidence = non_stereotype_votes / total_votes if total_votes > 0 else 0\n",
    "        \n",
    "        if identity_term not in all_religion_data:\n",
    "            all_religion_data[identity_term] = []\n",
    "\n",
    "        all_religion_data[identity_term].append({\n",
    "            'token': token,\n",
    "            'stereotype': stereotype,\n",
    "            'annotation_confidence': annotation_confidence,\n",
    "            'annotation': religion_token_annotations.get(token, None),\n",
    "        })\n",
    "\n",
    "json.dump(all_religion_data, open('./mlm_analysis/all_religion_data.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating scores for the experiments - Rough ground for testing things out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = './mlm_analysis/adv_future/results/region_results.json'\n",
    "test_data = json.load(open(test_file, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "stereotype_scores = {}\n",
    "stereotype_tokens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for identity_term in all_region_data:\n",
    "    stereotype_scores[identity_term] = {}\n",
    "    stereotype_tokens[identity_term] = {}\n",
    "    for token_data in all_region_data[identity_term]:\n",
    "        token = token_data['token']\n",
    "        annotation = token_data['annotation']\n",
    "        stereotype = token_data['stereotype']\n",
    "        annotation_confidence = token_data['annotation_confidence']\n",
    "        if stereotype and token in test_data[identity_term][annotation]:\n",
    "            if annotation not in stereotype_scores[identity_term]:\n",
    "                stereotype_scores[identity_term][annotation] = 0\n",
    "                stereotype_tokens[identity_term][annotation] = []\n",
    "            stereotype_scores[identity_term][annotation] += test_data[identity_term][annotation][token] * annotation_confidence\n",
    "            stereotype_tokens[identity_term][annotation].append(token)\n",
    "\n",
    "for identity_term in stereotype_scores:\n",
    "    stereotype_scores[identity_term] = {k: v for k, v in sorted(stereotype_scores[identity_term].items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arunachali': {},\n",
       " 'Assamese': {},\n",
       " 'Bengali': {'subjects': 0.3333333333333333},\n",
       " 'Bihari': {'adjectives': 0.3333333333333333},\n",
       " 'Chattisgarhi': {},\n",
       " 'Goan': {'subjects': 0.6666666666666666},\n",
       " 'Gujarati': {},\n",
       " 'Haryanvi': {'subjects': 0.3333333333333333},\n",
       " 'Himachali': {},\n",
       " 'Jharkhandi': {},\n",
       " 'Kannadiga': {'subjects': 1.0},\n",
       " 'Kashmiri': {},\n",
       " 'Madhya pradeshi': {},\n",
       " 'Manipuri': {'subjects': 0.3333333333333333},\n",
       " 'Marathi': {},\n",
       " 'Meghalayan': {},\n",
       " 'Mizo': {},\n",
       " 'Odiya': {},\n",
       " 'Punjabi': {'subjects': 0.3333333333333333},\n",
       " 'Rajasthani': {'subjects': 0.6666666666666666,\n",
       "  'adjectives': 0.3333333333333333},\n",
       " 'Tamilian': {'subjects': 1.6666666666666665},\n",
       " 'Telugu': {},\n",
       " 'Tripuri': {},\n",
       " 'Uttar pradeshi': {'professions': 0.6666666666666666},\n",
       " 'Uttarakhandi': {}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stereotype_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Arunachali': {},\n",
       " 'Assamese': {},\n",
       " 'Bengali': {'subjects': ['art']},\n",
       " 'Bihari': {'adjectives': ['poor']},\n",
       " 'Chattisgarhi': {},\n",
       " 'Goan': {'subjects': ['english']},\n",
       " 'Gujarati': {},\n",
       " 'Haryanvi': {'subjects': ['music']},\n",
       " 'Himachali': {},\n",
       " 'Jharkhandi': {},\n",
       " 'Kannadiga': {'subjects': ['english']},\n",
       " 'Kashmiri': {},\n",
       " 'Madhya pradeshi': {},\n",
       " 'Manipuri': {'subjects': ['art']},\n",
       " 'Marathi': {},\n",
       " 'Meghalayan': {},\n",
       " 'Mizo': {},\n",
       " 'Odiya': {},\n",
       " 'Punjabi': {'subjects': ['music']},\n",
       " 'Rajasthani': {'subjects': ['history', 'music'], 'adjectives': ['rich']},\n",
       " 'Tamilian': {'subjects': ['english', 'science']},\n",
       " 'Telugu': {},\n",
       " 'Tripuri': {},\n",
       " 'Uttar pradeshi': {'professions': ['farmer']},\n",
       " 'Uttarakhandi': {}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stereotype_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating these scores for all the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_framing\n",
      "negative_framing Muslim offensive_terms terrorist\n",
      "positive_framing\n",
      "positive_framing Muslim offensive_terms terrorist\n",
      "adv_inc\n",
      "adv_inc Muslim offensive_terms terrorist\n",
      "all_region_data.json\n",
      "adv_perspective_shift\n",
      "adv_perspective_shift Muslim offensive_terms terrorist\n",
      "adv_dec\n",
      "adv_dec Muslim offensive_terms terrorist\n",
      "region_annotated.csv\n",
      "adv_future\n",
      "adv_future Muslim offensive_terms terrorist\n",
      "vanilla\n",
      "vanilla Muslim offensive_terms terrorist\n",
      "combined\n",
      "religion_annotated.csv\n",
      "adv_present\n",
      "adv_present Muslim offensive_terms terrorist\n",
      "neutral_framing\n",
      "neutral_framing Muslim offensive_terms terrorist\n",
      "all_religion_data.json\n",
      "adv_past\n",
      "adv_past Muslim offensive_terms terrorist\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir('./mlm_analysis'):\n",
    "    print(folder)\n",
    "    if os.path.isdir(f'./mlm_analysis/{folder}'):\n",
    "        os.makedirs(f'./mlm_analysis/{folder}/final_results', exist_ok=True)\n",
    "\n",
    "        if 'combined' not in folder:\n",
    "            files_list = ['region_results.json']\n",
    "            for file in files_list:\n",
    "                file_path = os.path.join('./mlm_analysis', folder, 'results', file)\n",
    "\n",
    "                results_data = json.load(open(file_path, 'r'))\n",
    "\n",
    "                stereotype_scores = {}\n",
    "                stereotype_tokens = {}\n",
    "\n",
    "                for identity_term in all_region_data:\n",
    "                    stereotype_scores[identity_term] = {}\n",
    "                    stereotype_tokens[identity_term] = {}\n",
    "                    for token_data in all_region_data[identity_term]:\n",
    "                        token = token_data['token']\n",
    "                        annotation = token_data['annotation']\n",
    "                        stereotype = token_data['stereotype']\n",
    "                        annotation_confidence = token_data['annotation_confidence']\n",
    "                        try:\n",
    "                            if stereotype and token in results_data[identity_term][annotation]:\n",
    "                                if annotation not in stereotype_scores[identity_term]:\n",
    "                                    stereotype_scores[identity_term][annotation] = 0\n",
    "                                    stereotype_tokens[identity_term][annotation] = []\n",
    "                                stereotype_scores[identity_term][annotation] += results_data[identity_term][annotation][token] * annotation_confidence\n",
    "                                stereotype_tokens[identity_term][annotation].append(token)     \n",
    "                        except Exception as e:\n",
    "                            print(folder, identity_term, annotation, token)\n",
    "\n",
    "                with open(f'./mlm_analysis/{folder}/final_results/{file}', 'w') as f:\n",
    "                    json.dump(stereotype_scores, f, indent=4)\n",
    "                with open(f'./mlm_analysis/{folder}/final_results/stereotype_tokens_{file}', 'w') as f:\n",
    "                    json.dump(stereotype_tokens, f, indent=4)   \n",
    "\n",
    "            files_list = ['religion_results.json']\n",
    "            for file in files_list:\n",
    "                file_path = os.path.join('./mlm_analysis', folder, 'results', file)\n",
    "\n",
    "                results_data = json.load(open(file_path, 'r'))\n",
    "\n",
    "                stereotype_scores = {}\n",
    "                stereotype_tokens = {}\n",
    "\n",
    "                for identity_term in all_religion_data:\n",
    "                    stereotype_scores[identity_term] = {}\n",
    "                    stereotype_tokens[identity_term] = {}\n",
    "                    for token_data in all_religion_data[identity_term]:\n",
    "                        token = token_data['token']\n",
    "                        annotation = token_data['annotation']\n",
    "                        stereotype = token_data['stereotype']\n",
    "                        annotation_confidence = token_data['annotation_confidence']\n",
    "                        try:\n",
    "                            if stereotype and token in results_data[identity_term][annotation]:\n",
    "                                if annotation not in stereotype_scores[identity_term]:\n",
    "                                    stereotype_scores[identity_term][annotation] = 0\n",
    "                                    stereotype_tokens[identity_term][annotation] = []\n",
    "                                stereotype_scores[identity_term][annotation] += results_data[identity_term][annotation][token] * annotation_confidence\n",
    "                                stereotype_tokens[identity_term][annotation].append(token)     \n",
    "                        except Exception as e:\n",
    "                            print(folder, identity_term, annotation, token)\n",
    "\n",
    "                with open(f'./mlm_analysis/{folder}/final_results/{file}', 'w') as f:\n",
    "                    json.dump(stereotype_scores, f, indent=4)\n",
    "                with open(f'./mlm_analysis/{folder}/final_results/stereotype_tokens_{file}', 'w') as f:\n",
    "                    json.dump(stereotype_tokens, f, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
